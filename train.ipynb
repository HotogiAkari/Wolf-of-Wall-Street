{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 股票预测模型工作流\n",
                "\n",
                "---\n",
                "### 工作流说明\n",
                "1.  **阶段零 (Setup)**: 导入库、加载配置。\n",
                "2.  **阶段一 (Data Pipeline)**: 独立运行。负责处理并保存数据，生成 L2 特征数据缓存。\n",
                "3.  **阶段二 (Model Pipeline)**: 独立运行。包含三个子步骤：\n",
                "    - **2.1 HPO**: 自动调参。\n",
                "    - **2.2 (预处理)**: 智能地加载或生成 L3 预处理数据缓存\n",
                "    - **2.3 (模型训练)**: 使用 L3 缓存进行高效的模型训练。\n",
                "    - **2.4 (评估)**: 对训练结果进行聚合与可视化。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. 通用设置与导入"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Akari\\AppData\\Local\\Temp\\ipykernel_25072\\1534436633.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
                        "  from tqdm.autonotebook import tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO: 项目模型导入成功.\n",
                        "SUCCESS: 从 'configs/config.yaml' 加载 Config.\n"
                    ]
                }
            ],
            "source": [
                "import os, sys, yaml, torch, joblib, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "from tqdm.autonotebook import tqdm\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "os.environ['PYOPENCL_CTX'] = '0'\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "try:\n",
                "    from data_process.get_data import initialize_apis, shutdown_apis\n",
                "    from data_process.save_data import run_data_pipeline, get_processed_data_path\n",
                "    from model_builders.build_models import run_training_for_ticker, _walk_forward_split\n",
                "    from model_builders.hpo_utils import run_hpo_for_ticker\n",
                "    from model_builders.model_fuser import ModelFuser\n",
                "    from model_builders.lstm_builder import LSTMBuilder\n",
                "    print(\"INFO: 项目模型导入成功.\")\n",
                "except ImportError as e:\n",
                "    print(f\"WARNNING: 导入失败: {e}. 正在添加项目根目录...\")\n",
                "    project_root = str(Path().resolve()); sys.path.append(project_root) if project_root not in sys.path else None\n",
                "    from data_process.get_data import initialize_apis, shutdown_apis\n",
                "    from data_process.save_data import run_data_pipeline, get_processed_data_path\n",
                "    from model_builders.build_models import run_training_for_ticker, _walk_forward_split\n",
                "    from model_builders.hpo_utils import run_hpo_for_ticker\n",
                "    from model_builders.model_fuser import ModelFuser\n",
                "    from model_builders.lstm_builder import LSTMBuilder\n",
                "    print(\"INFO: 导入成功.\")\n",
                "\n",
                "CONFIG_PATH = 'configs/config.yaml'\n",
                "try:\n",
                "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f: config = yaml.safe_load(f)\n",
                "    print(f\"SUCCESS: 从 '{CONFIG_PATH}' 加载 Config.\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"ERROR: 未找到 Config.\"); config = {}\n",
                "\n",
                "if config:\n",
                "    global_settings, strategy_config, hpo_config, default_model_params, stocks_to_process = (\n",
                "        config.get('global_settings', {}), config.get('strategy_config', {}), \n",
                "        config.get('hpo_config', {}), config.get('default_model_params', {}), \n",
                "        config.get('stocks_to_process', [])\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **阶段一：数据准备与特征工程**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- 开始步骤 1: 数据准备 ---\n",
                        "\n",
                        "INFO: 尝试登陆 Baostock...\n",
                        "login success!\n",
                        "INFO: Baostock API 登录成功。SDK版本: 00.8.90\n",
                        "INFO: 未在配置中提供有效的 Tushare Token。将跳过宏观数据获取。\n",
                        "开始执行数据管道协调任务...\n",
                        "将使用配置文件: configs/config.yaml\n",
                        "INFO: 特征文件已存在于 data\\processed\\000001.SZ\\None_to_2025-09-30\\features_1ba8b04683b8.pkl，跳过 平安银行 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\601606.SH\\None_to_2025-09-30\\features_880856d3ad9a.pkl，跳过 长城军工 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000681.SZ\\None_to_2025-09-30\\features_77cd65ed35fb.pkl，跳过 视觉中国 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\603099.SH\\None_to_2025-09-30\\features_9846c606a4c6.pkl，跳过 长白山 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000100.SZ\\None_to_2025-09-30\\features_5867ad8766d5.pkl，跳过 TCL科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000426.SZ\\None_to_2025-09-30\\features_b88d378eac89.pkl，跳过 兴业矿业 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\002083.SZ\\None_to_2025-09-30\\features_facb951a04af.pkl，跳过 孚日股份 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000150.SZ\\None_to_2025-09-30\\features_dde6d8fe72c2.pkl，跳过 宜华健康 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\300013.SZ\\None_to_2025-09-30\\features_5867ad8766d5.pkl，跳过 新宁物流 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\300242.SZ\\None_to_2025-09-30\\features_f9f2d45b4160.pkl，跳过 佳云科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\600301.SH\\None_to_2025-09-30\\features_8a24a633c386.pkl，跳过 ST南化 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\002006.SZ\\None_to_2025-09-30\\features_f68a67a78faa.pkl，跳过 精功科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\300242.SZ\\None_to_2025-09-30\\features_f9f2d45b4160.pkl，跳过 佳云科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\002129.SZ\\None_to_2025-09-30\\features_ba18f5626ed2.pkl，跳过 TCL中环 的数据处理。\n",
                        "\n",
                        "所有股票的特征文件均已存在。无需执行数据处理流水线。\n",
                        "logout success!\n",
                        "INFO: Baostock API 已成功登出.\n"
                    ]
                }
            ],
            "source": [
                "print(\"--- 开始步骤 1: 数据准备 ---\\n\")\n",
                "try:\n",
                "    if config: initialize_apis(config); run_data_pipeline(config_path=CONFIG_PATH)\n",
                "    else: print(\"ERROR: Config 为空.\")\n",
                "finally:\n",
                "    shutdown_apis()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **阶段二：模型训练与评估**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 数据预加载与全局预处理 (L3 缓存)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- 开始步骤 2.2: 数据预加载与全局预处理 ---\\n\n",
                        "INFO: 未找到 L3 缓存或正在强制重新处理. 正在启动预处理...\\n\n",
                        "INFO: 未找到 L3 缓存或缓存为空. 正在启动预处理...\\n\n",
                        "INFO: PyTorch LSTMBuilder will use device: CUDA\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4d0b34fc466549f3bfb794b01705d4f8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Pre-processing Stocks:   0%|          | 0/14 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  - 对于 平安银行，已识别出 36 个用于模型输入的有效特征.\n"
                    ]
                }
            ],
            "source": [
                "global_data_cache = {}\n",
                "print(\"--- 开始步骤 2.2: 数据预加载与全局预处理 ---\\\\n\")\n",
                "\n",
                "L3_CACHE_DIR = Path(global_settings.get('output_dir', 'data/processed'))\n",
                "L3_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
                "L3_CACHE_PATH = L3_CACHE_DIR / \"_preprocessed_cache.joblib\"\n",
                "\n",
                "FORCE_REPROCESS = False      # 设置为 True 以重建缓存\n",
                "\n",
                "if L3_CACHE_PATH.exists() and not FORCE_REPROCESS:\n",
                "    print(f\"INFO: 已找到 L3 缓存. 正在从 {L3_CACHE_PATH} 加载...\")\n",
                "    try:\n",
                "        global_data_cache = joblib.load(L3_CACHE_PATH)\n",
                "        print(\"SUCCESS: L3 缓存已加载到内存中.\")\n",
                "    except Exception as e:\n",
                "        print(f\"WARNNING: 加载 L3 缓存失败: {e}. 正在启动预处理.\")\n",
                "        global_data_cache = {}\n",
                "else:\n",
                "    print(\"INFO: 未找到 L3 缓存或正在强制重新处理. 正在启动预处理...\\\\n\")\n",
                "\n",
                "if not global_data_cache:\n",
                "    print(\"INFO: 未找到 L3 缓存或缓存为空. 正在启动预处理...\\\\n\")\n",
                "    if config and stocks_to_process:\n",
                "        lstm_builder_for_preprocessing = LSTMBuilder(config)\n",
                "        \n",
                "        for stock_info in tqdm(stocks_to_process, desc=\"Pre-processing Stocks\"):\n",
                "            ticker = stock_info.get('ticker'); keyword = stock_info.get('keyword', ticker)\n",
                "            if not ticker: continue\n",
                "            data_path = get_processed_data_path(stock_info, config)\n",
                "            if not data_path.exists(): print(f\"\\\\nERROR: 未找到 {keyword} 的 L2 数据.\"); continue\n",
                "            \n",
                "            df = pd.read_pickle(data_path); df.index.name = 'date'\n",
                "            folds = _walk_forward_split(df, strategy_config)\n",
                "            if not folds: print(f\"\\\\nWARNNING: No folds for {keyword}.\"); continue\n",
                "\n",
                "            preprocessed_folds_lgbm, preprocessed_folds_lstm = [], []\n",
                "            label_col = global_settings.get('label_column', 'label_return')\n",
                "            features_for_model = [\n",
                "                c for c in df.columns \n",
                "                if c != label_col and not c.startswith('future_')\n",
                "            ]\n",
                "            print(f\"  - 对于 {keyword}，已识别出 {len(features_for_model)} 个用于模型输入的有效特征.\")\n",
                "\n",
                "            for train_df, val_df in folds:\n",
                "                # 1. 为 LGBM 准备数据 (不变)\n",
                "                X_train_model, y_train = train_df[features_for_model], train_df[label_col]\n",
                "                X_val_model, y_val = val_df[features_for_model], val_df[label_col]\n",
                "                scaler_lgbm = StandardScaler()\n",
                "                X_train_scaled = pd.DataFrame(scaler_lgbm.fit_transform(X_train_model), index=X_train_model.index, columns=features_for_model)\n",
                "                X_val_scaled = pd.DataFrame(scaler_lgbm.transform(X_val_model), index=X_val_model.index, columns=features_for_model)\n",
                "                preprocessed_folds_lgbm.append({'X_train_scaled': X_train_scaled, 'y_train': y_train, 'X_val_scaled': X_val_scaled, 'y_val': y_val})\n",
                "\n",
                "                # 2. 为 LSTM 准备数据\n",
                "                if 'lstm' in global_settings.get('models_to_train', []):\n",
                "                    # --- 核心修正：为验证集拼接历史数据 ---\n",
                "                    lstm_seq_len = lstm_builder_for_preprocessing.sequence_length\n",
                "                    \n",
                "                    # 确保 train_df 足够长\n",
                "                    if len(train_df) < lstm_seq_len: continue\n",
                "\n",
                "                    # 从 train_df 的末尾“借”一段历史\n",
                "                    train_history_for_val = train_df.iloc[-lstm_seq_len:]\n",
                "                    # 将历史和验证集合并，为 _create_sequences 提供连续的数据\n",
                "                    combined_df_for_lstm_val = pd.concat([train_history_for_val, val_df])\n",
                "                    # ---\n",
                "                    \n",
                "                    scaler_lstm = StandardScaler()\n",
                "                    train_df_scaled = train_df.copy(); combined_df_for_lstm_val_scaled = combined_df_for_lstm_val.copy()\n",
                "                    \n",
                "                    train_df_scaled[features_for_model] = scaler_lstm.fit_transform(train_df[features_for_model])\n",
                "                    combined_df_for_lstm_val_scaled[features_for_model] = scaler_lstm.transform(combined_df_for_lstm_val[features_for_model])\n",
                "\n",
                "                    X_train_seq, y_train_seq, _ = lstm_builder_for_preprocessing._create_sequences(train_df_scaled, features_for_model)\n",
                "                    \n",
                "                    # 使用拼接后的数据为验证集创建序列\n",
                "                    X_val_seq, y_val_seq, dates_val_seq = lstm_builder_for_preprocessing._create_sequences(combined_df_for_lstm_val_scaled, features_for_model)\n",
                "\n",
                "                    preprocessed_folds_lstm.append({'X_train_tensor': torch.from_numpy(X_train_seq), 'y_train_tensor': torch.from_numpy(y_train_seq).unsqueeze(1), 'X_val_tensor': torch.from_numpy(X_val_seq), 'y_val_tensor': torch.from_numpy(y_val_seq).unsqueeze(1), 'y_val_seq': y_val_seq, 'dates_val_seq': dates_val_seq})\n",
                "            \n",
                "            global_data_cache[ticker] = {'full_df': df, 'lgbm_folds': preprocessed_folds_lgbm, 'lstm_folds': preprocessed_folds_lstm}\n",
                "            print(f\"  - Cached {len(preprocessed_folds_lgbm)} folds for LGBM and {len(preprocessed_folds_lstm)} folds for LSTM for {keyword}.\")\n",
                "        \n",
                "        print(f\"\\\\nINFO: 预处理已完成. 正在将 L3 缓存保存至 {L3_CACHE_PATH}...\")\n",
                "        try:\n",
                "            joblib.dump(global_data_cache, L3_CACHE_PATH)\n",
                "            print(\"SUCCESS: L3 缓存已保存.\")\n",
                "        except Exception as e:\n",
                "            print(f\"ERROR: 保存 L3 缓存失败: {e}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 超参数优化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[I 2025-10-14 10:17:11,930] A new study created in memory with name: no-name-b688e9e1-918b-4f1b-82df-45b2af557a43\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- 开始为模型 ['lgbm', 'lstm'] 和股票 ['600519.SH', '000001.SZ'] 进行超参数优化 ---\n",
                        "\n",
                        "\n",
                        "################################################################################\n",
                        "# 开始为模型 [LGBM] 进行 HPO\n",
                        "################################################################################\n",
                        "\n",
                        "INFO: 已为 贵州茅台 加载最后 2 个 folds 用于 LGBM HPO。\n",
                        "\n",
                        "--- 开始为 贵州茅台 (600519.SH) 进行 LGBM HPO (共 50 轮) ---\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fb06a25ed372483d96e175dc6adf5c04",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/50 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    - Quantile 0.05: Finished. Best iter: [199851]\n",
                        "    - Quantile 0.5: Finished. Best iter: [199599]\n",
                        "    - Quantile 0.95: Finished. Best iter: [1]\n",
                        "    - Quantile 0.05: Finished. Best iter: [38]\n",
                        "    - Quantile 0.5: Finished. Best iter: [20]\n",
                        "    - Quantile 0.95: Finished. Best iter: [39]\n",
                        "[I 2025-10-14 10:24:18,785] Trial 0 finished with value: 3.4927912819681683 and parameters: {'num_leaves': 42, 'learning_rate': 0.07969454818643935, 'min_child_samples': 79, 'feature_fraction': 0.8394633936788146, 'bagging_fraction': 0.6624074561769746, 'reg_alpha': 0.029375384576328288, 'reg_lambda': 0.014936568554617643}. Best is trial 0 with value: 3.4927912819681683.\n",
                        "    - Quantile 0.05: Finished. Best iter: [190437]\n",
                        "    - Quantile 0.5: Finished. Best iter: [198423]\n",
                        "    - Quantile 0.95: Finished. Best iter: [1]\n",
                        "    - Quantile 0.05: Finished. Best iter: [18]\n",
                        "    - Quantile 0.5: Finished. Best iter: [20]\n",
                        "    - Quantile 0.95: Finished. Best iter: [38]\n",
                        "[I 2025-10-14 10:31:19,341] Trial 1 finished with value: 3.5611305545896554 and parameters: {'num_leaves': 72, 'learning_rate': 0.015930522616241012, 'min_child_samples': 77, 'feature_fraction': 0.608233797718321, 'bagging_fraction': 0.9879639408647978, 'reg_alpha': 3.142880890840109, 'reg_lambda': 0.04335281794951567}. Best is trial 1 with value: 3.5611305545896554.\n",
                        "    - Quantile 0.05: Finished. Best iter: [190437]\n",
                        "    - Quantile 0.5: Finished. Best iter: [198423]\n",
                        "    - Quantile 0.95: Finished. Best iter: [1]\n",
                        "    - Quantile 0.05: Finished. Best iter: [38]\n",
                        "    - Quantile 0.5: Finished. Best iter: [20]\n",
                        "    - Quantile 0.95: Finished. Best iter: [38]\n",
                        "[I 2025-10-14 10:38:15,838] Trial 2 finished with value: 3.668317502037031 and parameters: {'num_leaves': 31, 'learning_rate': 0.002327067708383781, 'min_child_samples': 44, 'feature_fraction': 0.8099025726528951, 'bagging_fraction': 0.7727780074568463, 'reg_alpha': 0.07476312062252301, 'reg_lambda': 0.6847920095574779}. Best is trial 2 with value: 3.668317502037031.\n",
                        "    - Quantile 0.05: Finished. Best iter: [199851]\n",
                        "    - Quantile 0.5: Finished. Best iter: [198001]\n",
                        "    - Quantile 0.95: Finished. Best iter: [1]\n",
                        "    - Quantile 0.05: Finished. Best iter: [18]\n",
                        "    - Quantile 0.5: Finished. Best iter: [20]\n",
                        "    - Quantile 0.95: Finished. Best iter: [39]\n",
                        "[I 2025-10-14 10:45:13,752] Trial 3 finished with value: 3.622030874597113 and parameters: {'num_leaves': 28, 'learning_rate': 0.00383962929980417, 'min_child_samples': 49, 'feature_fraction': 0.7824279936868144, 'bagging_fraction': 0.9140703845572055, 'reg_alpha': 0.039721107273819126, 'reg_lambda': 0.34890188454913873}. Best is trial 2 with value: 3.668317502037031.\n",
                        "    - Quantile 0.05: Finished. Best iter: [198423]\n",
                        "    - Quantile 0.5: Finished. Best iter: [199974]\n",
                        "[W 2025-10-14 10:49:24,546] Trial 4 failed with parameters: {'num_leaves': 56, 'learning_rate': 0.001238513729886093, 'min_child_samples': 69, 'feature_fraction': 0.6682096494749166, 'bagging_fraction': 0.6260206371941118, 'reg_alpha': 7.025166339242156, 'reg_lambda': 7.886714129990489} because of the following error: KeyboardInterrupt().\n",
                        "Traceback (most recent call last):\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
                        "    value_or_values = func(trial)\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\hpo_utils.py\", line 113, in <lambda>\n",
                        "    lambda trial: objective(trial, preprocessed_folds, config, model_type),\n",
                        "                  ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\hpo_utils.py\", line 78, in objective\n",
                        "    _, ic_series, _ = builder.train_and_evaluate_fold(\n",
                        "                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
                        "        train_df=None, val_df=None, cached_data=fold_data\n",
                        "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "    )\n",
                        "    ^\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\lgbm_builder.py\", line 57, in train_and_evaluate_fold\n",
                        "    model.fit(\n",
                        "    ~~~~~~~~~^\n",
                        "        X_train_scaled, y_train,\n",
                        "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "    ...<2 lines>...\n",
                        "        callbacks=[early_stopping(stopping_rounds=early_stopping_rounds, verbose=False)]\n",
                        "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "    )\n",
                        "    ^\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1398, in fit\n",
                        "    super().fit(\n",
                        "    ~~~~~~~~~~~^\n",
                        "        X,\n",
                        "        ^^\n",
                        "    ...<11 lines>...\n",
                        "        init_model=init_model,\n",
                        "        ^^^^^^^^^^^^^^^^^^^^^^\n",
                        "    )\n",
                        "    ^\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1049, in fit\n",
                        "    self._Booster = train(\n",
                        "                    ~~~~~^\n",
                        "        params=params,\n",
                        "        ^^^^^^^^^^^^^^\n",
                        "    ...<6 lines>...\n",
                        "        callbacks=callbacks,\n",
                        "        ^^^^^^^^^^^^^^^^^^^^\n",
                        "    )\n",
                        "    ^\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\engine.py\", line 322, in train\n",
                        "    booster.update(fobj=fobj)\n",
                        "    ~~~~~~~~~~~~~~^^^^^^^^^^^\n",
                        "  File \"d:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\basic.py\", line 4155, in update\n",
                        "    _LIB.LGBM_BoosterUpdateOneIter(\n",
                        "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
                        "        self._handle,\n",
                        "        ^^^^^^^^^^^^^\n",
                        "        ctypes.byref(is_finished),\n",
                        "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "    )\n",
                        "    ^\n",
                        "KeyboardInterrupt\n",
                        "[W 2025-10-14 10:49:24,551] Trial 4 failed with value None.\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mINFO: 已为 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 加载最后 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(hpo_folds_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 个 folds 用于 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type_for_hpo.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m HPO。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m hpo_run_config = {\n\u001b[32m     53\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mglobal_settings\u001b[39m\u001b[33m'\u001b[39m: global_settings, \u001b[33m'\u001b[39m\u001b[33mstrategy_config\u001b[39m\u001b[33m'\u001b[39m: strategy_config,\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdefault_model_params\u001b[39m\u001b[33m'\u001b[39m: default_model_params, \u001b[33m'\u001b[39m\u001b[33mstocks_to_process\u001b[39m\u001b[33m'\u001b[39m: [stock_info],\n\u001b[32m     55\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhpo_config\u001b[39m\u001b[33m'\u001b[39m: hpo_config\n\u001b[32m     56\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m best_params, best_value = \u001b[43mrun_hpo_for_ticker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocessed_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpo_folds_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhpo_run_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type_for_hpo\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# n_trials 会在 run_hpo_for_ticker 函数内部自动获取\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_params \u001b[38;5;129;01mand\u001b[39;00m best_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m     hpo_results_list.append({\u001b[33m'\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m'\u001b[39m: ticker, \u001b[33m'\u001b[39m\u001b[33mkeyword\u001b[39m\u001b[33m'\u001b[39m: keyword, \u001b[33m'\u001b[39m\u001b[33mbest_score\u001b[39m\u001b[33m'\u001b[39m: best_value, **best_params})\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\hpo_utils.py:112\u001b[39m, in \u001b[36mrun_hpo_for_ticker\u001b[39m\u001b[34m(preprocessed_folds, ticker, config, model_type)\u001b[39m\n\u001b[32m    109\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m, storage=\u001b[38;5;28;01mNone\u001b[39;00m, sampler=optuna.samplers.TPESampler(seed=config.get(\u001b[33m'\u001b[39m\u001b[33mglobal_settings\u001b[39m\u001b[33m'\u001b[39m, {}).get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m42\u001b[39m)))\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[32m    119\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m错误: 在为 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 进行 HPO 时发生异常: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\hpo_utils.py:113\u001b[39m, in \u001b[36mrun_hpo_for_ticker.<locals>.<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    109\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m, storage=\u001b[38;5;28;01mNone\u001b[39;00m, sampler=optuna.samplers.TPESampler(seed=config.get(\u001b[33m'\u001b[39m\u001b[33mglobal_settings\u001b[39m\u001b[33m'\u001b[39m, {}).get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m42\u001b[39m)))\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    112\u001b[39m     study.optimize(\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessed_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m    114\u001b[39m         n_trials=n_trials, \n\u001b[32m    115\u001b[39m         n_jobs=\u001b[32m1\u001b[39m, \n\u001b[32m    116\u001b[39m         show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[32m    119\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m错误: 在为 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 进行 HPO 时发生异常: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\hpo_utils.py:78\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial, preprocessed_folds, config, model_type)\u001b[39m\n\u001b[32m     75\u001b[39m warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# 忽略此 with 块内的所有警告\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# 调用 builder 的训练和评估方法，并正确解包 3 个返回值\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m _, ic_series, _ = \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_evaluate_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold_data\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ic_series \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ic_series.empty:\n\u001b[32m     83\u001b[39m     ic_scores.append(ic_series[\u001b[33m'\u001b[39m\u001b[33mrank_ic\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m])\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\model_builders\\lgbm_builder.py:57\u001b[39m, in \u001b[36mLGBMBuilder.train_and_evaluate_fold\u001b[39m\u001b[34m(self, train_df, val_df, cached_data)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m     56\u001b[39m     warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquantile\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m best_iteration = model.best_iteration_\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lgbm_params.get(\u001b[33m'\u001b[39m\u001b[33mverbose_period\u001b[39m\u001b[33m'\u001b[39m, -\u001b[32m1\u001b[39m) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m best_iteration:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1383\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1395\u001b[39m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1396\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLGBMRegressor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Project\\Command\\Python\\Neural\\Wolf_of_Wall_Street\\.venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# train.ipynb -> \"2.2 (可选) 超参数优化\" (最终完整版)\n",
                "\n",
                "RUN_HPO = False  # 设为 True 以运行优化，False 则跳过\n",
                "\n",
                "# --- 定义要进行 HPO 的模型列表 ---\n",
                "MODELS_FOR_HPO = ['lgbm', 'lstm']\n",
                "\n",
                "if RUN_HPO and config:\n",
                "    hpo_tickers = hpo_config.get('tickers_for_hpo', [])\n",
                "    \n",
                "    if not hpo_tickers:\n",
                "        print(\"INFO: 在配置文件中未指定用于 HPO 的股票，跳过此步骤。\")\n",
                "    elif 'global_data_cache' not in locals() or not global_data_cache:\n",
                "        print(\"ERROR: 全局数据缓存 (global_data_cache) 为空。请先成功运行 2.1 预处理单元格。\")\n",
                "    else:\n",
                "        print(f\"--- 开始为模型 {MODELS_FOR_HPO} 和股票 {hpo_tickers} 进行超参数优化 ---\\n\")\n",
                "        \n",
                "        # 循环遍历要优化的每个模型类型\n",
                "        for model_type_for_hpo in MODELS_FOR_HPO:\n",
                "            print(f\"# 开始为模型 [{model_type_for_hpo.upper()}] 进行 HPO\")\n",
                "            \n",
                "            hpo_results_list = []\n",
                "            \n",
                "            # --- 从模型专属配置或全局配置中获取 hpo_num_eval_folds ---\n",
                "            model_hpo_config = hpo_config.get(f'{model_type_for_hpo}_hpo_config', {})\n",
                "            num_eval_folds = model_hpo_config.get('hpo_num_eval_folds', hpo_config.get('hpo_num_eval_folds', 2))\n",
                "\n",
                "            for ticker in hpo_tickers:\n",
                "                stock_info = next((s for s in stocks_to_process if s['ticker'] == ticker), None)\n",
                "                if not stock_info:\n",
                "                    print(f\"WARNNING: 未在 'stocks_to_process' 中找到 HPO 股票 {ticker} 的配置。跳过。\")\n",
                "                    continue\n",
                "                \n",
                "                keyword = stock_info.get('keyword', ticker)\n",
                "\n",
                "                if ticker not in global_data_cache:\n",
                "                    print(f\"ERROR: 预处理数据缓存中未找到 {keyword} 的数据。请先成功运行 2.1 单元格。跳过。\")\n",
                "                    continue\n",
                "\n",
                "                all_preprocessed_folds = global_data_cache[ticker].get(f'{model_type_for_hpo}_folds', [])\n",
                "                if not all_preprocessed_folds:\n",
                "                    print(f\"WARNNING: 缓存中未找到 {keyword} 的 '{model_type_for_hpo}' 预处理数据。跳过 HPO。\")\n",
                "                    continue\n",
                "                \n",
                "                # 使用正确的 num_eval_folds 来切片数据\n",
                "                hpo_folds_data = all_preprocessed_folds[-num_eval_folds:]\n",
                "                \n",
                "                print(f\"\\nINFO: 已为 {keyword} 加载最后 {len(hpo_folds_data)} 个 folds 用于 {model_type_for_hpo.upper()} HPO。\")\n",
                "\n",
                "                hpo_run_config = {\n",
                "                    'global_settings': global_settings, 'strategy_config': strategy_config,\n",
                "                    'default_model_params': default_model_params, 'stocks_to_process': [stock_info],\n",
                "                    'hpo_config': hpo_config\n",
                "                }\n",
                "                \n",
                "                best_params, best_value = run_hpo_for_ticker(\n",
                "                    preprocessed_folds=hpo_folds_data,\n",
                "                    ticker=ticker,\n",
                "                    config=hpo_run_config,\n",
                "                    model_type=model_type_for_hpo\n",
                "                    # n_trials 会在 run_hpo_for_ticker 函数内部自动获取\n",
                "                )\n",
                "                \n",
                "                if best_params and best_value is not None:\n",
                "                    hpo_results_list.append({'ticker': ticker, 'keyword': keyword, 'best_score': best_value, **best_params})\n",
                "            \n",
                "            if hpo_results_list:\n",
                "                # --- 实现“冠军排行榜”更新逻辑 ---\n",
                "                hpo_log_dir = Path(\"hpo_logs\")\n",
                "                hpo_log_dir.mkdir(exist_ok=True)\n",
                "                # 每个模型一个独立的冠军榜文件\n",
                "                hpo_best_results_path = hpo_log_dir / f\"hpo_best_results_{model_type_for_hpo}.csv\"\n",
                "                \n",
                "                current_hpo_df = pd.DataFrame(hpo_results_list).set_index('ticker')\n",
                "\n",
                "                if hpo_best_results_path.exists():\n",
                "                    print(f\"\\nINFO: 正在加载 [{model_type_for_hpo.upper()}] 的历史最佳 HPO 结果...\")\n",
                "                    historical_best_df = pd.read_csv(hpo_best_results_path).set_index('ticker')\n",
                "                    \n",
                "                    for ticker, current_row in current_hpo_df.iterrows():\n",
                "                        if ticker not in historical_best_df.index or current_row['best_score'] > historical_best_df.loc[ticker, 'best_score']:\n",
                "                            keyword = current_row.get('keyword', ticker)\n",
                "                            historical_score = historical_best_df.loc[ticker, 'best_score'] if ticker in historical_best_df.index else 'N/A'\n",
                "                            print(f\"  - 新纪录! [{model_type_for_hpo.upper()}] {keyword} 的最佳分数从 {historical_score if isinstance(historical_score, str) else f'{historical_score:.4f}'} 提升至 {current_row['best_score']:.4f}.\")\n",
                "                            historical_best_df.loc[ticker] = current_row\n",
                "                    final_best_df = historical_best_df\n",
                "                else:\n",
                "                    print(f\"\\nINFO: 未找到 [{model_type_for_hpo.upper()}] 的历史 HPO 结果，将本次结果作为初始最佳记录。\")\n",
                "                    final_best_df = current_hpo_df\n",
                "\n",
                "                final_best_df.to_csv(hpo_best_results_path)\n",
                "                print(f\"SUCCESS: 最新的 [{model_type_for_hpo.upper()}] HPO 冠军榜已保存至 {hpo_best_results_path}\")\n",
                "                \n",
                "                # --- 展示和应用最终的参数 ---\n",
                "                PARAM_MAP_CN = {'best_score': '最佳分数 (ICIR)', 'keyword': '股票名称', 'num_leaves': '叶子节点数', 'learning_rate': '学习率', 'min_child_samples': '叶节点最小样本数', 'feature_fraction': '特征采样比例', 'bagging_fraction': '数据采样比例', 'reg_alpha': 'L1正则化', 'reg_lambda': 'L2正则化', 'units_1': '隐藏层1单元数', 'units_2': '隐藏层2单元数', 'dropout': 'Dropout率'}\n",
                "                display_df = final_best_df.reset_index().rename(columns=PARAM_MAP_CN)\n",
                "                if '股票名称' in display_df.columns: display_df = display_df.set_index(['ticker', '股票名称'])\n",
                "                \n",
                "                print(f\"--- {model_type_for_hpo.upper()} HPO 最佳参数冠军榜 ---\")\n",
                "                display(display_df.style.format({'最佳分数 (ICIR)': '{:.4f}'}).background_gradient(cmap='viridis', subset=['最佳分数 (ICIR)']))\n",
                "                \n",
                "                param_cols_original = [c for c in hpo_results_list[0].keys() if c not in ['ticker', 'keyword', 'best_score']]\n",
                "                final_hpo_params = final_best_df[param_cols_original].mean().to_dict()\n",
                "                average_best_score = final_best_df['best_score'].mean()\n",
                "                \n",
                "                for p in ['num_leaves', 'min_child_samples', 'units_1', 'units_2']:\n",
                "                    if p in final_hpo_params: final_hpo_params[p] = int(round(final_hpo_params[p]))\n",
                "                \n",
                "                param_key = f\"{model_type_for_hpo}_params\"\n",
                "                config['default_model_params'][param_key].update(final_hpo_params)\n",
                "                default_model_params[param_key] = config['default_model_params'][param_key]\n",
                "                \n",
                "                print(f\"--- {model_type_for_hpo.upper()} HPO 综合结果 ---\")\n",
                "                print(f\"本轮 HPO 冠军榜平均最高分 (ICIR): {average_best_score:.4f}\")\n",
                "                print(f\"将用于后续训练的【{model_type_for_hpo.upper()} 平均参数】如下:\")\n",
                "                print(yaml.dump(default_model_params[param_key], allow_unicode=True))\n",
                "\n",
                "else:\n",
                "    print(\"INFO: 跳过 HPO 步骤。\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 模型训练"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FORCE_RETRAIN = True # 您可以在这里控制是否强制重训所有模型\n",
                "all_ic_history = []\n",
                "\n",
                "print(\"--- Starting Stage 2.3: Model Training ---\\\\n\")\n",
                "print(f\"INFO: Force re-train for base models is set to: {FORCE_RETRAIN}\")\n",
                "\n",
                "if config and stocks_to_process:\n",
                "    models_to_train = global_settings.get('models_to_train', ['lgbm', 'lstm'])\n",
                "    stock_iterator = tqdm(stocks_to_process, desc=\"Processing Stocks\")\n",
                "\n",
                "    for stock_info in stock_iterator:\n",
                "        ticker = stock_info.get('ticker')\n",
                "        if not ticker or ticker not in global_data_cache:\n",
                "            continue\n",
                "        \n",
                "        keyword = stock_info.get('keyword', ticker)\n",
                "        stock_iterator.set_description(f\"Processing {keyword}\")\n",
                "        \n",
                "        cached_stock_data = global_data_cache[ticker]\n",
                "        full_df = cached_stock_data['full_df']\n",
                "        \n",
                "        for model_type in models_to_train:\n",
                "            \n",
                "            # --- 核心修正：在这里实现 FORCE_RETRAIN 的检查逻辑 ---\n",
                "            model_dir = Path(global_settings.get('model_dir', 'models')) / ticker\n",
                "            file_suffixes = {'lgbm': '.pkl', 'lstm': '.pt'}\n",
                "            model_suffix = file_suffixes.get(model_type, '.pkl')\n",
                "            existing_models = sorted(model_dir.glob(f\"{model_type}_model_*{model_suffix}\"))\n",
                "            \n",
                "            # 检查 IC 历史文件，它更能代表一次完整的训练是否已完成\n",
                "            ic_history_path = model_dir / f\"{model_type}_ic_history.csv\"\n",
                "\n",
                "            if ic_history_path.exists() and not FORCE_RETRAIN:\n",
                "                print(f\"\\\\nINFO: Found existing IC history for {keyword} [{model_type.upper()}]. Skipping training.\")\n",
                "                # 即使跳过训练，也要加载历史 IC 用于最终评估\n",
                "                try:\n",
                "                    ic_history = pd.read_csv(ic_history_path, index_col='date', parse_dates=True)\n",
                "                    all_ic_history.append(ic_history)\n",
                "                except Exception as e:\n",
                "                    print(f\"  - WARNNING: Failed to load existing IC history: {e}\")\n",
                "                continue # 跳到下一个 model_type\n",
                "            # --- 修正结束 ---\n",
                "\n",
                "            folds_key = f\"{model_type}_folds\"\n",
                "            preprocessed_folds = cached_stock_data.get(folds_key)\n",
                "            \n",
                "            if not preprocessed_folds:\n",
                "                print(f\"\\\\nWARNNING: 未找到 {keyword} 模型 '{model_type}' 的预处理 folds. 跳过.\")\n",
                "                continue\n",
                "\n",
                "            run_config = {\n",
                "                'global_settings': global_settings, 'strategy_config': strategy_config,\n",
                "                'default_model_params': default_model_params, 'stocks_to_process': [stock_info],\n",
                "                'full_df_for_final_model': full_df\n",
                "            }\n",
                "\n",
                "            ic_history = run_training_for_ticker(\n",
                "                preprocessed_folds=preprocessed_folds,\n",
                "                ticker=ticker,\n",
                "                model_type=model_type,\n",
                "                config=run_config, \n",
                "                force_retrain=FORCE_RETRAIN, # 这个参数在函数内部仍然有用，作为双重保险\n",
                "                keyword=keyword\n",
                "            )\n",
                "            \n",
                "            if ic_history is not None and not ic_history.empty:\n",
                "                all_ic_history.append(ic_history)\n",
                "else:\n",
                "    print(\"ERROR: Config 或 stocks_to_process 为空.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3.5 融合模型训练"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FORCE_FUSER_RETRAIN = True\n",
                "\n",
                "if config and stocks_to_process:\n",
                "    fuser_iterator = tqdm(stocks_to_process, desc=\"Training Fusers\")\n",
                "    for stock_info in fuser_iterator:\n",
                "        ticker = stock_info.get('ticker')\n",
                "        keyword = stock_info.get('keyword', ticker)\n",
                "        fuser_iterator.set_description(f\"Training Fuser for {keyword}\")\n",
                "        if not ticker: continue\n",
                "\n",
                "        run_config = {\n",
                "            'global_settings': global_settings, \n",
                "            'strategy_config': strategy_config,\n",
                "            'default_model_params': default_model_params,\n",
                "            'stocks_to_process': [stock_info]\n",
                "        }\n",
                "        fuser = ModelFuser(ticker, run_config)\n",
                "        \n",
                "        # --- 核心修正 2：在训练前，检查是否需要跳过 ---\n",
                "        # 如果不强制重训，并且 fuser_meta.json 文件已存在，则跳过\n",
                "        if not FORCE_FUSER_RETRAIN and fuser.meta_path.exists():\n",
                "            print(f\"INFO: Fusion model meta for {keyword} already exists. Skipping training.\")\n",
                "            continue\n",
                "        # ---\n",
                "        \n",
                "        # --- 核心修正 3：在 fuser.train() 调用前，打印股票名 ---\n",
                "        # (这需要我们在 ModelFuser.train() 中移除第一行打印)\n",
                "        print(f\"\\n--- 正在为 {keyword} ({ticker}) 训练融合元模型... ---\")\n",
                "        fuser.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 结果聚合、评估与可视化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- 开始步骤 2.4: 结果聚合、评估与可视化 ---\")\n",
                "if all_ic_history:\n",
                "    full_ic_df = pd.concat(all_ic_history)\n",
                "    full_ic_df['ticker_name'] = full_ic_df['ticker'].map({s['ticker']: s.get('keyword', s['ticker']) for s in stocks_to_process})\n",
                "    \n",
                "    # 聚合评估结果\n",
                "    evaluation_summary = full_ic_df.groupby(['ticker_name', 'model_type'])['rank_ic'].agg(['mean', 'std']).reset_index()\n",
                "    evaluation_summary['icir'] = evaluation_summary['mean'] / evaluation_summary['std']\n",
                "    \n",
                "    # --- 1. 打印和显示评估表格 ---\n",
                "    print(\"\\n--- 模型性能评估总结 ---\")\n",
                "    display(evaluation_summary.style.format({\n",
                "        'mean': '{:.4f}', 'std': '{:.4f}', 'icir': '{:.4f}'\n",
                "    }).background_gradient(cmap='viridis', subset=['icir']))\n",
                "\n",
                "    # --- 2. 绘制 ICIR 对比图 ---\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.barplot(data=evaluation_summary, x='ticker_name', y='icir', hue='model_type')\n",
                "    plt.title('模型信息比率 (ICIR) 对比', fontsize=16)\n",
                "    plt.xlabel('股票', fontsize=12)\n",
                "    plt.ylabel('ICIR (信息比率)', fontsize=12)\n",
                "    plt.axhline(0, color='grey', linestyle='--')\n",
                "    plt.axhline(0.5, color='red', linestyle='--', label='ICIR=0.5 (良好)')\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    # --- 3. 绘制累积 IC 曲线图 ---\n",
                "    plot_df = full_ic_df.copy()\n",
                "    plot_df['date'] = pd.to_datetime(plot_df['date'])\n",
                "    plot_df.sort_values('date', inplace=True)\n",
                "    plot_df['cumulative_ic'] = plot_df.groupby(['ticker_name', 'model_type'])['rank_ic'].cumsum()\n",
                "    \n",
                "    plt.figure(figsize=(14, 8))\n",
                "    sns.lineplot(data=plot_df, x='date', y='cumulative_ic', hue='ticker_name', style='model_type', marker='o', markersize=4, linestyle='--')\n",
                "    plt.title('模型累积 Rank IC 曲线', fontsize=16)\n",
                "    plt.xlabel('日期', fontsize=12)\n",
                "    plt.ylabel('累积 Rank IC', fontsize=12)\n",
                "    plt.legend(title='股票/模型')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "else:\n",
                "    print(\"\\nWARNNING: 训练期间未生成 IC 历史。跳过汇总和评估.\")"
            ]
        }
    ],
    "metadata": {
        "...": "...",
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
