{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 股票预测模型工作流\n",
                "\n",
                "---\n",
                "### 工作流说明\n",
                "1.  **阶段零 (Setup)**: 导入库、加载配置。\n",
                "2.  **阶段一 (Data Pipeline)**: 独立运行。负责处理并保存数据，生成 L2 特征数据缓存。\n",
                "3.  **阶段二 (Model Pipeline)**: 独立运行。包含三个子步骤：\n",
                "    - **2.1 HPO**: 自动调参。\n",
                "    - **2.2 (预处理)**: 智能地加载或生成 L3 预处理数据缓存\n",
                "    - **2.3 (模型训练)**: 使用 L3 缓存进行高效的模型训练。\n",
                "    - **2.4 (评估)**: 对训练结果进行聚合与可视化。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. 通用设置与导入"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Akari\\AppData\\Local\\Temp\\ipykernel_37944\\824614192.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
                        "  from tqdm.autonotebook import tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO: 项目模型导入成功.\n",
                        "SUCCESS: 从 'configs/config.yaml' 加载 Config.\n"
                    ]
                }
            ],
            "source": [
                "import os, sys, yaml, torch, joblib, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "from tqdm.autonotebook import tqdm\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "os.environ['PYOPENCL_CTX'] = '0'\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "try:\n",
                "    from data_process.get_data import initialize_apis, shutdown_apis\n",
                "    from data_process.save_data import run_data_pipeline, get_processed_data_path\n",
                "    from model_builders.build_models import run_training_for_ticker, _walk_forward_split\n",
                "    from model_builders.hpo_utils import run_hpo_for_ticker\n",
                "    from model_builders.model_fuser import ModelFuser\n",
                "    from model_builders.lstm_builder import LSTMBuilder\n",
                "    print(\"INFO: 项目模型导入成功.\")\n",
                "except ImportError as e:\n",
                "    print(f\"WARNNING: 导入失败: {e}. 正在添加项目根目录...\")\n",
                "    project_root = str(Path().resolve()); sys.path.append(project_root) if project_root not in sys.path else None\n",
                "    from data_process.get_data import initialize_apis, shutdown_apis\n",
                "    from data_process.save_data import run_data_pipeline, get_processed_data_path\n",
                "    from model_builders.build_models import run_training_for_ticker, _walk_forward_split\n",
                "    from model_builders.hpo_utils import run_hpo_for_ticker\n",
                "    from model_builders.model_fuser import ModelFuser\n",
                "    from model_builders.lstm_builder import LSTMBuilder\n",
                "    print(\"INFO: 导入成功.\")\n",
                "\n",
                "CONFIG_PATH = 'configs/config.yaml'\n",
                "try:\n",
                "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f: config = yaml.safe_load(f)\n",
                "    print(f\"SUCCESS: 从 '{CONFIG_PATH}' 加载 Config.\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"ERROR: 未找到 Config.\"); config = {}\n",
                "\n",
                "if config:\n",
                "    global_settings, strategy_config, hpo_config, default_model_params, stocks_to_process = (\n",
                "        config.get('global_settings', {}), config.get('strategy_config', {}), \n",
                "        config.get('hpo_config', {}), config.get('default_model_params', {}), \n",
                "        config.get('stocks_to_process', [])\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **阶段一：数据准备与特征工程**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- 开始步骤 1: 数据准备 ---\n",
                        "\n",
                        "INFO: 尝试登陆 Baostock...\n",
                        "login success!\n",
                        "INFO: Baostock API 登录成功。SDK版本: 00.8.90\n",
                        "INFO: 未在配置中提供有效的 Tushare Token。将跳过宏观数据获取。\n",
                        "开始执行数据管道协调任务...\n",
                        "将使用配置文件: configs/config.yaml\n",
                        "INFO: 特征文件已存在于 data\\processed\\000001.SZ\\None_to_2025-09-30\\features_3426a81a0a66.pkl，跳过 平安银行 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000100.SZ\\None_to_2025-09-30\\features_a496eedfeef4.pkl，跳过 TCL科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000426.SZ\\None_to_2025-09-30\\features_7735d66ba2d8.pkl，跳过 兴业矿业 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\002083.SZ\\None_to_2025-09-30\\features_31355922e534.pkl，跳过 孚日股份 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\000150.SZ\\None_to_2025-09-30\\features_d3643490c4b0.pkl，跳过 宜华健康 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\300013.SZ\\None_to_2025-09-30\\features_a496eedfeef4.pkl，跳过 新宁物流 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\300242.SZ\\None_to_2025-09-30\\features_4f6e73a5f824.pkl，跳过 佳云科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\002006.SZ\\None_to_2025-09-30\\features_665dc9104f40.pkl，跳过 精功科技 的数据处理。\n",
                        "INFO: 特征文件已存在于 data\\processed\\300242.SZ\\None_to_2025-09-30\\features_4f6e73a5f824.pkl，跳过 佳云科技 的数据处理。\n",
                        "\n",
                        "需要为以下 1 只股票生成新数据: ['600301.SH']\n",
                        "--- 开始批量特征生成 ---\n",
                        "针对特定股票: 1 生成特征.\n",
                        "\n",
                        "--- Generating features for ST南化 (600301.SH) ---\n",
                        "  - Running in Training Mode: Fetching historical data based on config.\n",
                        "  - Data window: Requesting data from 2010-09-30 to 2025-09-30.\n",
                        "  - [1/7] 正在从本地缓存加载 sh.600301 的原始日线数据...\n",
                        "  - INFO: Received data for ST南化 from 2010-09-30 to 2025-09-30.\n",
                        "INFO: Starting feature calculation pipeline...\n",
                        "  - [Calculating Features] INFO: No technical indicators specified in config. Skipping Technical Indicators.\n",
                        "  - [Calculating Features] Running: Calendar Features...\n",
                        "  - [Calculating Features] INFO: No candlestick patterns specified in config. Skipping Candlestick Patterns.\n",
                        "  - [Calculating Features] Running: Statistical Features...\n",
                        "  - [Calculating Features] Running: Price Structure Features...\n",
                        "  - [Calculating Features] Running: Volume Features...\n",
                        "  - [Calculating Features] Running: Trend Regime Features...\n",
                        "INFO: Feature calculation pipeline finished.\n",
                        "  - [1/7] 正在从本地缓存加载 sh.000984 的原始日线数据...\n",
                        "  - [1/7] 正在从 Baostock 下载 sh.512690 的日线行情...\n",
                        "  - WARNING [BS]: 未能获取到 sh.512690 在指定日期范围的数据。\n",
                        "WARNNING: Could not get industry data for ST南化. Using benchmark as fallback.\n",
                        "  - [4/7] 正在添加相对表现特征...\n",
                        "  - [5/7] 正在对特征进行平稳化...\n",
                        "  - [6/7] 正在创建并降噪预测标签...\n",
                        "  - [7/7] 正在进行初步特征筛选...\n",
                        "    - 移除了 5 个高相关性特征: ['week_of_year', 'industry_close', 'relative_strength_vs_industry', 'correlation_vs_industry', 'industry_close_log_return']\n",
                        "INFO: 运行架构验证...\n",
                        "    - WARNNING (Data Gaps): Found 6 large gap(s) in time series after the first data point. Max gap: 107 days 00:00:00.\n",
                        "ERROR: 数据结构验证失败!\n",
                        "验证失败详情:\n",
                        "False\n",
                        "ERROR: Final data validation failed for ST南化. Aborting.\n",
                        "--- Batch Feature Generation Process Finished ---\n",
                        "未能从数据管道获取任何新数据，跳过保存步骤。\n",
                        "\n",
                        "数据管道协调任务执行完毕。\n",
                        "logout success!\n",
                        "INFO: Baostock API 已成功登出.\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    if config: initialize_apis(config); run_data_pipeline(config_path=CONFIG_PATH)\n",
                "    else: print(\"ERROR: Config 为空.\")\n",
                "finally:\n",
                "    shutdown_apis()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **阶段二：模型训练与评估**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 数据预加载与全局预处理 (L3 缓存)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO: L3 cache not found or is empty. Starting pre-processing...\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f5419b2f44a8403d961688415bb55551",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Pre-processing Stocks:   0%|          | 0/10 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  - Cached 104 folds for LGBM and 104 folds for LSTM for 平安银行.\n",
                        "  - Cached 98 folds for LGBM and 98 folds for LSTM for TCL科技.\n",
                        "  - Cached 95 folds for LGBM and 95 folds for LSTM for 兴业矿业.\n",
                        "  - Cached 104 folds for LGBM and 104 folds for LSTM for 孚日股份.\n",
                        "  - Cached 76 folds for LGBM and 76 folds for LSTM for 宜华健康.\n",
                        "  - Cached 98 folds for LGBM and 98 folds for LSTM for 新宁物流.\n",
                        "  - Cached 94 folds for LGBM and 94 folds for LSTM for 佳云科技.\n",
                        "\n",
                        "ERROR: L2 data for ST南化 not found. Skipping pre-processing.\n",
                        "  - Cached 93 folds for LGBM and 93 folds for LSTM for 精功科技.\n",
                        "  - Cached 94 folds for LGBM and 94 folds for LSTM for 佳云科技.\n",
                        "\n",
                        "INFO: Pre-processing finished. Saving L3 cache to data\\processed\\_preprocessed_cache.joblib...\n",
                        "SUCCESS: L3 cache saved.\n",
                        "\n",
                        "--- Stage 2.1 Finished: All data is cached in memory. ---\n"
                    ]
                }
            ],
            "source": [
                "FORCE_REPROCESS = True     # 是否重新处理数据\n",
                "\n",
                "global_data_cache = {}\n",
                "\n",
                "L3_CACHE_DIR = Path(global_settings.get('output_dir', 'data/processed'))\n",
                "L3_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
                "L3_CACHE_PATH = L3_CACHE_DIR / \"_preprocessed_cache.joblib\"\n",
                "\n",
                "if L3_CACHE_PATH.exists() and not FORCE_REPROCESS:\n",
                "    print(f\"INFO: 已找到 L3 缓存. 正在从 {L3_CACHE_PATH} 加载...\")\n",
                "    try:\n",
                "        global_data_cache = joblib.load(L3_CACHE_PATH)\n",
                "        print(\"SUCCESS: 已将 L3 缓存加载入内存.\")\n",
                "    except Exception as e:\n",
                "        print(f\"WARNNING: 加载 L3 缓存失败: {e}. 将重新预处理数据.\")\n",
                "        global_data_cache = {}\n",
                "\n",
                "if not global_data_cache:\n",
                "    print(\"INFO: 未找到 L3 缓存或为空. 开始重新预处理数据...\\n\")\n",
                "    if config and stocks_to_process:\n",
                "        lstm_builder_for_preprocessing = LSTMBuilder(config)\n",
                "        \n",
                "        for stock_info in tqdm(stocks_to_process, desc=\"Pre-processing Stocks\"):\n",
                "            ticker = stock_info.get('ticker'); keyword = stock_info.get('keyword', ticker)\n",
                "            if not ticker: continue\n",
                "            data_path = get_processed_data_path(stock_info, config)\n",
                "            if not data_path.exists():\n",
                "                print(f\"\\nERROR: 未找到 {keyword} 的 L2 数据, 跳过预处理.\")\n",
                "                continue\n",
                "            \n",
                "            df = pd.read_pickle(data_path); df.index.name = 'date'\n",
                "            folds = _walk_forward_split(df, strategy_config)\n",
                "            if not folds:\n",
                "                print(f\"\\nWARNNING: 未为 {keyword} 生成 folds. 跳过预处理.\")\n",
                "                continue\n",
                "\n",
                "            preprocessed_folds_lgbm, preprocessed_folds_lstm = [], []\n",
                "            label_col = global_settings.get('label_column', 'label_alpha')\n",
                "            features_for_model = [c for c in df.columns if c != label_col and not c.startswith('future_')]\n",
                "\n",
                "            for train_df, val_df in folds:\n",
                "                X_train_model, y_train = train_df[features_for_model], train_df[label_col]\n",
                "                X_val_model, y_val = val_df[features_for_model], val_df[label_col]\n",
                "                scaler_lgbm = StandardScaler()\n",
                "                X_train_scaled = pd.DataFrame(scaler_lgbm.fit_transform(X_train_model), index=X_train_model.index, columns=features_for_model)\n",
                "                X_val_scaled = pd.DataFrame(scaler_lgbm.transform(X_val_model), index=X_val_model.index, columns=features_for_model)\n",
                "                preprocessed_folds_lgbm.append({'X_train_scaled': X_train_scaled, 'y_train': y_train, 'X_val_scaled': X_val_scaled, 'y_val': y_val})\n",
                "\n",
                "                # --- 实现层级覆盖逻辑 ---\n",
                "                use_lstm_for_this_stock = stock_info.get('use_lstm') \n",
                "                if use_lstm_for_this_stock is None:\n",
                "                    use_lstm_for_this_stock = global_settings.get('use_lstm_globally', True)\n",
                "                \n",
                "                if 'lstm' in global_settings.get('models_to_train', []) and use_lstm_for_this_stock:\n",
                "                    lstm_seq_len = lstm_builder_for_preprocessing.sequence_length\n",
                "                    if len(train_df) < lstm_seq_len: continue\n",
                "                    train_history_for_val = train_df.iloc[-lstm_seq_len:]\n",
                "                    combined_df_for_lstm_val = pd.concat([train_history_for_val, val_df])\n",
                "                    \n",
                "                    scaler_lstm = StandardScaler()\n",
                "                    train_df_scaled = train_df.copy(); combined_df_for_lstm_val_scaled = combined_df_for_lstm_val.copy()\n",
                "                    train_df_scaled[features_for_model] = scaler_lstm.fit_transform(train_df[features_for_model])\n",
                "                    combined_df_for_lstm_val_scaled[features_for_model] = scaler_lstm.transform(combined_df_for_lstm_val[features_for_model])\n",
                "\n",
                "                    X_train_seq, y_train_seq, _ = lstm_builder_for_preprocessing._create_sequences(train_df_scaled, features_for_model)\n",
                "                    X_val_seq, y_val_seq, dates_val_seq = lstm_builder_for_preprocessing._create_sequences(combined_df_for_lstm_val_scaled, features_for_model)\n",
                "\n",
                "                    lstm_precision = default_model_params.get('lstm_params', {}).get('precision', 32)\n",
                "                    torch_dtype = torch.float16 if lstm_precision == 16 else torch.float32\n",
                "                    preprocessed_folds_lstm.append({'X_train_tensor': torch.from_numpy(X_train_seq).to(dtype=torch_dtype), 'y_train_tensor': torch.from_numpy(y_train_seq).unsqueeze(1).to(dtype=torch_dtype), 'X_val_tensor': torch.from_numpy(X_val_seq).to(dtype=torch_dtype), 'y_val_tensor': torch.from_numpy(y_val_seq).unsqueeze(1).to(dtype=torch_dtype), 'y_val_seq': y_val_seq, 'dates_val_seq': dates_val_seq})\n",
                "            \n",
                "            global_data_cache[ticker] = {'full_df': df, 'lgbm_folds': preprocessed_folds_lgbm, 'lstm_folds': preprocessed_folds_lstm}\n",
                "            print(f\"  - 已为  {keyword} 缓存 {len(preprocessed_folds_lgbm)} 个 LGBM folds 和 {len(preprocessed_folds_lstm)} 个 LSTM folds.\")\n",
                "        \n",
                "        print(f\"\\nINFO: 预处理完成. 正在将 L3 缓存保存至 {L3_CACHE_PATH}...\")\n",
                "        try:\n",
                "            joblib.dump(global_data_cache, L3_CACHE_PATH)\n",
                "            print(\"SUCCESS: L3 缓存已保存.\")\n",
                "        except Exception as e:\n",
                "            print(f\"ERROR: 保存 L3 缓存失败: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 超参数优化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO: 跳过 HPO 步骤。\n"
                    ]
                }
            ],
            "source": [
                "RUN_HPO = False # 设为 True 以运行优化，False 则跳过\n",
                "\n",
                "if RUN_HPO and config:\n",
                "    MODELS_FOR_HPO = ['lgbm', 'lstm']\n",
                "    \n",
                "    hpo_tickers = hpo_config.get('tickers_for_hpo', [])\n",
                "    \n",
                "    if not hpo_tickers:\n",
                "        print(\"INFO: 在配置文件中未指定用于 HPO 的股票，跳过此步骤。\")\n",
                "    elif 'global_data_cache' not in locals() or not global_data_cache:\n",
                "        print(\"ERROR: 全局数据缓存 (global_data_cache) 为空。请先成功运行 2.1 预处理单元格。\")\n",
                "    else:\n",
                "        print(f\"--- 开始为模型 {MODELS_FOR_HPO} 和股票 {hpo_tickers} 进行超参数优化 ---\\n\")\n",
                "        \n",
                "        # 循环遍历要优化的每个模型类型\n",
                "        for model_type_for_hpo in MODELS_FOR_HPO:\n",
                "            print(f\"\\n\" + \"#\"*80)\n",
                "            print(f\"# 开始为模型 [{model_type_for_hpo.upper()}] 进行 HPO\")\n",
                "            print(\"#\"*80)\n",
                "            \n",
                "            hpo_results_list = []\n",
                "            \n",
                "            model_hpo_config = hpo_config.get(f'{model_type_for_hpo}_hpo_config', {})\n",
                "            num_eval_folds = model_hpo_config.get('hpo_num_eval_folds', hpo_config.get('hpo_num_eval_folds', 2))\n",
                "\n",
                "            for ticker in hpo_tickers:\n",
                "                stock_info = next((s for s in stocks_to_process if s['ticker'] == ticker), None)\n",
                "                if not stock_info:\n",
                "                    print(f\"WARNNING: 未在 'stocks_to_process' 中找到 HPO 股票 {ticker} 的配置。跳过。\")\n",
                "                    continue\n",
                "                \n",
                "                keyword = stock_info.get('keyword', ticker)\n",
                "\n",
                "                use_lstm_for_this_stock = stock_info.get('use_lstm')\n",
                "                if use_lstm_for_this_stock is None:\n",
                "                    use_lstm_for_this_stock = global_settings.get('use_lstm_globally', True)\n",
                "                \n",
                "                if model_type_for_hpo == 'lstm' and not use_lstm_for_this_stock:\n",
                "                    print(f\"\\nINFO: {keyword} 已配置为不使用 LSTM，跳过 LSTM 的 HPO。\")\n",
                "                    continue\n",
                "\n",
                "                if ticker not in global_data_cache:\n",
                "                    print(f\"ERROR: 预处理数据缓存中未找到 {keyword} 的数据。跳过。\")\n",
                "                    continue\n",
                "\n",
                "                all_preprocessed_folds = global_data_cache[ticker].get(f'{model_type_for_hpo}_folds', [])\n",
                "                if not all_preprocessed_folds:\n",
                "                    print(f\"WARNNING: 缓存中未找到 {keyword} 的 '{model_type_for_hpo}' 预处理数据。跳过 HPO。\")\n",
                "                    continue\n",
                "                \n",
                "                hpo_folds_data = all_preprocessed_folds[-num_eval_folds:]\n",
                "                \n",
                "                print(f\"\\nINFO: 已为 {keyword} 加载最后 {len(hpo_folds_data)} 个 folds 用于 {model_type_for_hpo.upper()} HPO。\")\n",
                "\n",
                "                hpo_run_config = {\n",
                "                    'global_settings': global_settings, 'strategy_config': strategy_config,\n",
                "                    'default_model_params': default_model_params, 'stocks_to_process': [stock_info],\n",
                "                    'hpo_config': hpo_config\n",
                "                }\n",
                "                \n",
                "                best_params, best_value = run_hpo_for_ticker(\n",
                "                    preprocessed_folds=hpo_folds_data,\n",
                "                    ticker=ticker,\n",
                "                    config=hpo_run_config,\n",
                "                    model_type=model_type_for_hpo\n",
                "                )\n",
                "                \n",
                "                if best_params and best_value is not None:\n",
                "                    hpo_results_list.append({'ticker': ticker, 'keyword': keyword, 'best_score': best_value, **best_params})\n",
                "            \n",
                "            if hpo_results_list:\n",
                "                hpo_log_dir = Path(\"hpo_logs\"); hpo_log_dir.mkdir(exist_ok=True)\n",
                "                hpo_best_results_path = hpo_log_dir / f\"hpo_best_results_{model_type_for_hpo}.csv\"\n",
                "                \n",
                "                current_hpo_df = pd.DataFrame(hpo_results_list).set_index('ticker')\n",
                "\n",
                "                if hpo_best_results_path.exists():\n",
                "                    print(f\"\\nINFO: 正在加载 [{model_type_for_hpo.upper()}] 的历史最佳 HPO 结果...\")\n",
                "                    historical_best_df = pd.read_csv(hpo_best_results_path).set_index('ticker')\n",
                "                    \n",
                "                    for ticker, current_row in current_hpo_df.iterrows():\n",
                "                        if ticker not in historical_best_df.index or current_row['best_score'] > historical_best_df.loc[ticker, 'best_score']:\n",
                "                            keyword = current_row.get('keyword', ticker)\n",
                "                            historical_score = historical_best_df.loc[ticker, 'best_score'] if ticker in historical_best_df.index else 'N/A'\n",
                "                            print(f\"  - 新纪录! [{model_type_for_hpo.upper()}] {keyword} 的最佳分数从 {historical_score if isinstance(historical_score, str) else f'{historical_score:.4f}'} 提升至 {current_row['best_score']:.4f}.\")\n",
                "                            historical_best_df.loc[ticker] = current_row\n",
                "                    final_best_df = historical_best_df\n",
                "                else:\n",
                "                    print(f\"\\nINFO: 未找到 [{model_type_for_hpo.upper()}] 的历史 HPO 结果，将本次结果作为初始最佳记录。\")\n",
                "                    final_best_df = current_hpo_df\n",
                "\n",
                "                final_best_df.to_csv(hpo_best_results_path)\n",
                "                print(f\"SUCCESS: 最新的 [{model_type_for_hpo.upper()}] HPO 冠军榜已保存至 {hpo_best_results_path}\")\n",
                "                \n",
                "                PARAM_MAP_CN = {'best_score': '最佳分数 (ICIR)', 'keyword': '股票名称', 'num_leaves': '叶子节点数', 'learning_rate': '学习率', 'min_child_samples': '叶节点最小样本数', 'feature_fraction': '特征采样比例', 'bagging_fraction': '数据采样比例', 'reg_alpha': 'L1正则化', 'reg_lambda': 'L2正则化', 'units_1': '隐藏层1单元数', 'units_2': '隐藏层2单元数', 'dropout': 'Dropout率'}\n",
                "                display_df = final_best_df.reset_index().rename(columns=PARAM_MAP_CN)\n",
                "                if '股票名称' in display_df.columns: display_df = display_df.set_index(['ticker', '股票名称'])\n",
                "                \n",
                "                print(\"\\n\" + \"=\"*80)\n",
                "                print(f\"--- {model_type_for_hpo.upper()} HPO 最佳参数冠军榜 ---\")\n",
                "                display(display_df.style.format({'最佳分数 (ICIR)': '{:.4f}'}).background_gradient(cmap='viridis', subset=['最佳分数 (ICIR)']))\n",
                "                \n",
                "                param_cols_original = [c for c in hpo_results_list[0].keys() if c not in ['ticker', 'keyword', 'best_score']]\n",
                "                final_hpo_params = final_best_df[param_cols_original].mean().to_dict()\n",
                "                average_best_score = final_best_df['best_score'].mean()\n",
                "                \n",
                "                for p in ['num_leaves', 'min_child_samples', 'units_1', 'units_2']:\n",
                "                    if p in final_hpo_params: final_hpo_params[p] = int(round(final_hpo_params[p]))\n",
                "                \n",
                "                param_key = f\"{model_type_for_hpo}_params\"\n",
                "                config['default_model_params'][param_key].update(final_hpo_params)\n",
                "                default_model_params[param_key] = config['default_model_params'][param_key]\n",
                "                \n",
                "                print(f\"--- {model_type_for_hpo.upper()} HPO 综合结果 ---\")\n",
                "                print(f\"本轮 HPO 冠军榜平均最高分 (ICIR): {average_best_score:.4f}\")\n",
                "                print(f\"将用于后续训练的【{model_type_for_hpo.upper()} 平均参数】如下:\")\n",
                "                print(yaml.dump(default_model_params[param_key], allow_unicode=True))\n",
                "\n",
                "else:\n",
                "    print(\"INFO: 跳过 HPO 步骤。\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 模型训练"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO: 强制重新训练基础模型设置为：False\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "973b93900f5b4031a1e81e93aa6688df",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processing Stocks:   0%|          | 0/10 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "================================================================================\n",
                        "--- Starting LGBM training for 平安银行 (000001.SZ) ---\n",
                        "INFO: 开始对 平安银行 进行跨 104 folds 的前向验证...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5fb4e3b9c9884dab881a51d2d8fd4de3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "正在 平安银行 上训练 LGBM :   0%|          | 0/104 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# 2.3 模型训练\n",
                "\n",
                "FORCE_RETRAIN = False # 是否重新训练模型\n",
                "all_ic_history = []\n",
                "\n",
                "print(f\"INFO: 强制重新训练基础模型设置为：{FORCE_RETRAIN}\")\n",
                "\n",
                "if config and stocks_to_process:\n",
                "    models_to_train = global_settings.get('models_to_train', ['lgbm', 'lstm'])\n",
                "    stock_iterator = tqdm(stocks_to_process, desc=\"Processing Stocks\")\n",
                "\n",
                "    for stock_info in stock_iterator:\n",
                "        ticker = stock_info.get('ticker')\n",
                "        if not ticker or ticker not in global_data_cache:\n",
                "            continue\n",
                "        \n",
                "        keyword = stock_info.get('keyword', ticker)\n",
                "        stock_iterator.set_description(f\"Processing {keyword}\")\n",
                "        \n",
                "        cached_stock_data = global_data_cache[ticker]\n",
                "        full_df = cached_stock_data['full_df']\n",
                "        \n",
                "        for model_type in models_to_train:\n",
                "            use_lstm_for_this_stock = stock_info.get('use_lstm')\n",
                "            if use_lstm_for_this_stock is None:\n",
                "                use_lstm_for_this_stock = global_settings.get('use_lstm_globally', True)\n",
                "            if model_type == 'lstm' and not use_lstm_for_this_stock:\n",
                "                print(f\"INFO: {keyword} 已配置为不使用 LSTM, 跳过.\")\n",
                "                continue\n",
                "\n",
                "            model_dir = Path(global_settings.get('model_dir', 'models')) / ticker\n",
                "            ic_history_path = model_dir / f\"{model_type}_ic_history.csv\"\n",
                "            # 我们也检查模型文件本身\n",
                "            file_suffixes = {'lgbm': '.pkl', 'lstm': '.pt'}\n",
                "            model_files = list(model_dir.glob(f\"{model_type}_model_*{file_suffixes[model_type]}\"))\n",
                "\n",
                "            if ic_history_path.exists() and model_files and not FORCE_RETRAIN:\n",
                "                print(f\"\\nINFO: 已为 {keyword} [{model_type.upper()}] 找到现有模型和 IC 历史记录. 跳过训练.\")\n",
                "                try:\n",
                "                    ic_history = pd.read_csv(ic_history_path, index_col='date', parse_dates=True)\n",
                "                    all_ic_history.append(ic_history)\n",
                "                except Exception as e:\n",
                "                    print(f\"  - WARNNING: 无法加载现有的 IC 历史记录: {e}\")\n",
                "                continue\n",
                "\n",
                "            folds_key = f\"{model_type}_folds\"\n",
                "            preprocessed_folds = cached_stock_data.get(folds_key)\n",
                "            if not preprocessed_folds:\n",
                "                print(f\"\\nWARNNING: 未找到 {keyword} 的 '{model_type}' 的预处理 folds. 跳过.\")\n",
                "                continue\n",
                "\n",
                "            run_config = {\n",
                "                'global_settings': global_settings, \n",
                "                'strategy_config': strategy_config,\n",
                "                'default_model_params': default_model_params, \n",
                "                'stocks_to_process': [stock_info],\n",
                "                'full_df_for_final_model': full_df\n",
                "            }\n",
                "\n",
                "            ic_history = run_training_for_ticker(\n",
                "                preprocessed_folds=preprocessed_folds,\n",
                "                ticker=ticker,\n",
                "                model_type=model_type,\n",
                "                config=run_config, \n",
                "                force_retrain=FORCE_RETRAIN,\n",
                "                keyword=keyword\n",
                "            )\n",
                "            \n",
                "            if ic_history is not None and not ic_history.empty:\n",
                "                all_ic_history.append(ic_history)\n",
                "else:\n",
                "    print(\"ERROR: Config 或 stocks_to_process 为空.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3.5 融合模型训练"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FORCE_FUSER_RETRAIN = True\n",
                "\n",
                "if config and stocks_to_process:\n",
                "    fuser_iterator = tqdm(stocks_to_process, desc=\"Training Fusers\")\n",
                "    for stock_info in fuser_iterator:\n",
                "        ticker = stock_info.get('ticker')\n",
                "        keyword = stock_info.get('keyword', ticker)\n",
                "        fuser_iterator.set_description(f\"Training Fuser for {keyword}\")\n",
                "        if not ticker: continue\n",
                "\n",
                "        run_config = {\n",
                "            'global_settings': global_settings, \n",
                "            'strategy_config': strategy_config,\n",
                "            'default_model_params': default_model_params,\n",
                "            'stocks_to_process': [stock_info]\n",
                "        }\n",
                "        fuser = ModelFuser(ticker, run_config)\n",
                "        \n",
                "        # 如果不强制重训，并且 fuser_meta.json 文件已存在，则跳过\n",
                "        if not FORCE_FUSER_RETRAIN and fuser.meta_path.exists():\n",
                "            print(f\"INFO: Fusion model meta for {keyword} already exists. Skipping training.\")\n",
                "            continue\n",
                "\n",
                "        print(f\"\\n--- 正在为 {keyword} ({ticker}) 训练融合元模型... ---\")\n",
                "        fuser.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 结果聚合、评估与可视化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\\\n--- Stage 2.4: Aggregating, Fusing, and Visualizing Results ---\\n\")\n",
                "\n",
                "if all_ic_history:\n",
                "    # 1. 准备基础数据，并确保对于每个 (ticker, model_type) 组合，日期是唯一的\n",
                "    full_ic_df = pd.concat(all_ic_history).drop_duplicates(subset=['ticker', 'model_type', 'date'], keep='last')\n",
                "    full_ic_df['ticker_name'] = full_ic_df['ticker'].map({s['ticker']: s.get('keyword', s['ticker']) for s in stocks_to_process})\n",
                "    \n",
                "    # 2. 模拟融合模型的表现\n",
                "    fusion_ic_list = []\n",
                "    for ticker, group_df in full_ic_df.groupby('ticker'):\n",
                "        try:\n",
                "            pivot_df = group_df.pivot(index='date', columns='model_type', values='rank_ic')\n",
                "            if 'lgbm' not in pivot_df.columns or 'lstm' not in pivot_df.columns: continue\n",
                "            \n",
                "            pivot_df.dropna(inplace=True)\n",
                "            if len(pivot_df) < 2: continue # 至少需要2个重叠点才能有意义\n",
                "            \n",
                "            span = strategy_config.get('fusion_ic_span', 120)\n",
                "            rolling_ic_lgbm = abs(pivot_df['lgbm']).ewm(span=span, adjust=False).mean()\n",
                "            rolling_ic_lstm = abs(pivot_df['lstm']).ewm(span=span, adjust=False).mean()\n",
                "            total_rolling_ic = rolling_ic_lgbm + rolling_ic_lstm\n",
                "            w_lgbm = (rolling_ic_lgbm / total_rolling_ic).fillna(0.5)\n",
                "            w_lstm = 1 - w_lgbm\n",
                "            \n",
                "            pivot_df['fusion'] = (pivot_df['lgbm'] * w_lgbm) + (pivot_df['lstm'] * w_lstm)\n",
                "            \n",
                "            fusion_ic_stock_df = pivot_df[['fusion']].rename(columns={'fusion': 'rank_ic'}).reset_index()\n",
                "            fusion_ic_stock_df['ticker'] = ticker\n",
                "            fusion_ic_stock_df['model_type'] = 'FUSION'\n",
                "            fusion_ic_stock_df['ticker_name'] = group_df['ticker_name'].iloc[0]\n",
                "            fusion_ic_list.append(fusion_ic_stock_df)\n",
                "        except Exception as e:\n",
                "            print(f\"WARNNING: 为 {ticker} 计算融合模型 IC 时出错: {e}\")\n",
                "\n",
                "    # 3. 合并所有数据\n",
                "    final_eval_df = pd.concat([full_ic_df] + fusion_ic_list, ignore_index=True) if fusion_ic_list else full_ic_df\n",
                "    \n",
                "    # --- 4. 聚合与 ICIR 计算 ---\n",
                "    def safe_std(x):\n",
                "        return x.std(ddof=0) if len(x) > 1 else 0.0\n",
                "\n",
                "    evaluation_summary = final_eval_df.groupby(['ticker_name', 'model_type'])['rank_ic'].agg(\n",
                "        mean='mean',\n",
                "        std=safe_std\n",
                "    ).reset_index()\n",
                "\n",
                "    # 只有在 std > 0 时才计算 icir，否则为 0 或由均值决定\n",
                "    evaluation_summary['icir'] = np.where(\n",
                "        evaluation_summary['std'] > 1e-8, \n",
                "        evaluation_summary['mean'] / evaluation_summary['std'], \n",
                "        evaluation_summary['mean'] * 100 # 如果 std=0, 说明表现极度稳定, 给一个由均值决定的高分\n",
                "    )\n",
                "    \n",
                "    # --- 5. 可视化 ---\n",
                "    print(\"\\n--- ICIR 对比图 (缩放至合理范围) ---\")\n",
                "    \n",
                "    # 【1. 定义更专业的颜色】\n",
                "    # 使用一套更现代、区分度更高的颜色\n",
                "    custom_palette = {\n",
                "        \"lgbm\": \"#49b6ff\",\n",
                "        \"lstm\": \"#ffa915\",\n",
                "        \"FUSION\": \"#2ecc71\"\n",
                "    }\n",
                "\n",
                "    # 【2. 创建画布并绘制核心图表】\n",
                "    # 增大 figsize 的宽度，让每个股票的柱子有更多空间\n",
                "    fig, ax = plt.subplots(figsize=(20, 10))\n",
                "    sns.barplot(\n",
                "        data=evaluation_summary, \n",
                "        x='ticker_name', \n",
                "        y='icir', \n",
                "        hue='model_type',\n",
                "        palette=custom_palette, # 使用自定义颜色\n",
                "        ax=ax\n",
                "    )\n",
                "\n",
                "    # 【3. 为每个柱子添加精确的数值标签】\n",
                "    for p in ax.patches:\n",
                "        height = p.get_height()\n",
                "        if np.isnan(height): continue # 跳过 NaN 值\n",
                "        \n",
                "        # 根据柱子的高度，决定标签的位置（在柱顶上方或柱底下方）\n",
                "        y_offset = 0.03 * (2 - (-2)) # 动态计算偏移量，为 Y 轴范围的 3%\n",
                "        y_pos = height + y_offset if height >= 0 else height - y_offset\n",
                "        va = 'bottom' if height >= 0 else 'top'\n",
                "        \n",
                "        ax.text(\n",
                "            p.get_x() + p.get_width() / 2., # X 轴位置：柱子中心\n",
                "            y_pos,                          # Y 轴位置：在柱顶/底的上方/下方\n",
                "            f'{height:.2f}',                # 标签文本：格式化为两位小数\n",
                "            ha='center',                    # 水平居中\n",
                "            va=va,                          # 垂直对齐\n",
                "            fontsize=10,\n",
                "            color='dimgray',\n",
                "            fontweight='semibold'\n",
                "        )\n",
                "\n",
                "    # 【4. 优化图表美学细节】\n",
                "    REASONABLE_ICIR_RANGE = [-2.0, 2.0]\n",
                "    ax.set_ylim(REASONABLE_ICIR_RANGE)\n",
                "    ax.set_title(f'模型信息比率 (ICIR) 对比 - 缩放视图 (Y轴范围: {REASONABLE_ICIR_RANGE})', fontsize=20, fontweight='bold', pad=20)\n",
                "    ax.set_xlabel('股票', fontsize=14, fontweight='bold')\n",
                "    ax.set_ylabel('ICIR (信息比率)', fontsize=14, fontweight='bold')\n",
                "    \n",
                "    # 优化网格线\n",
                "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    ax.grid(axis='x', linestyle='', alpha=0) # 关闭垂直网格线\n",
                "    \n",
                "    # 优化坐标轴标签\n",
                "    ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
                "    ax.tick_params(axis='y', labelsize=12)\n",
                "    \n",
                "    # 移除顶部和右侧的边框\n",
                "    sns.despine(ax=ax)\n",
                "    \n",
                "    # 优化图例\n",
                "    ax.legend(title='模型类型', fontsize=12, title_fontsize=13, loc='upper left')\n",
                "    \n",
                "    # 添加 ICIR=0.5 的参考线\n",
                "    ax.axhline(0.5, color='red', linestyle='--', label='ICIR=0.5 (良好)')\n",
                "    \n",
                "    # 重新绘制图例以包含参考线\n",
                "    handles, labels = ax.get_legend_handles_labels()\n",
                "    ax.legend(handles=handles, labels=labels, title='模型类型', fontsize=12, title_fontsize=13, loc='upper left')\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    \n",
                "    # 绘制累积 IC 曲线图，使用 final_eval_df\n",
                "    plot_df = final_eval_df.copy()\n",
                "    plot_df['date'] = pd.to_datetime(plot_df['date'])\n",
                "    plot_df.sort_values('date', inplace=True)\n",
                "    # 确保 groupby 的列存在\n",
                "    if 'ticker_name' in plot_df.columns and 'model_type' in plot_df.columns:\n",
                "        plot_df['cumulative_ic'] = plot_df.groupby(['ticker_name', 'model_type'])['rank_ic'].cumsum()\n",
                "        \n",
                "        plt.figure(figsize=(16, 9))\n",
                "        sns.lineplot(data=plot_df, x='date', y='cumulative_ic', hue='ticker_name', style='model_type', marker='o', markersize=4, linestyle='--')\n",
                "        plt.title('模型累积 Rank IC 曲线 (含融合模型)', fontsize=16)\n",
                "        plt.xlabel('日期', fontsize=12); plt.ylabel('累积 Rank IC', fontsize=12)\n",
                "        plt.legend(title='股票/模型'); plt.tight_layout(); plt.show()\n",
                "\n",
                "else:\n",
                "    print(\"\\nWARNNING: 训练期间未生成 IC 历史。跳过汇总和评估。\")"
            ]
        }
    ],
    "metadata": {
        "...": "...",
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
